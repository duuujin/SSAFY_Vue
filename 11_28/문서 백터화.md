# 문서 백터화

- 문서(텍스트)를 컴퓨터가 연산 할 수 있는 수치 벡터로 바꾸는 과정
- 필요성
  - 컴퓨터는 문자열 자체를 수학적 연산에 바로 쓸 수 없음
  - 문서의 내용/구조/의미를 수치(벡터)로 옮겨야만, 검색/추천/분류 알고리즘을 적용할 수 있음
- 대표적인 벡터화
  - 카운트 기반
  - 임베딩 기반
    - Word2Vec
    - Doc2Vec
    - 생성형 AI 기반 임베딩

# 카운트 기반 벡터화

- 문서를 벡터로 바꿀 때, 단어가 얼마나 자주 나오는 지 를 이용하는 방법
- 말뭉치(corpus, 문서 모음)내 모든 단어로 사전을 만들고, 각 문서는 단어마다 단어 등장 횟수(Term Frequency, TF)를 세어 벡터로 표현하는 방법
- 대표 기법
  - BoW(Bag of Words) : 순서/문맥 무시, 등장 횟수만 기록
  - TF-IDF(TF-Inverse Document Frquency) : 흔한 단어 가중치 감소

## Bag of Words

- 고유 단어 추출
  - 불용어(조사, 접속사 등)를 제거 -> 더 높은 정확도
- 각 단어에 인덱스 부여
  - 사전순 혹은 등장 순서대로 인덱스를 부여
- 문서 별 각 단어의 등장 빈도수 기록
  - 문서 내 등장 횟수를 해당 단어의 인덱스 위치에 기록

# 카운트 기반 벡터화의 한계점

- 문서(단어) 벡터간 유의미한 유사도를 계산할 수 없음
  - 단어의 출현 횟수를 수로 작성할 뿐, 단어의 의미 자체를 전혀 반영하지 않음
  - Ex) '자동차' 와 '차'는 같은 의미이지만, 다른 단어로 취급
- '사과'와 '바나나'는 과일이라는 공통점을 가지고 있으나, 해당 정보를 단어 카운트 행렬에서는 반영할 수 있는 방법이 없음 => 즉, 과일이라는 공통점을 연관시키지 못함

- 어휘 크기만큼 벡터 길이가 커지고, 메모리/연산 부담이 큼

# 워드 임베딩

- 텍스트를 '의미'를 반영한 숫자 벡터로 변환하는 기법

## Word Embedding

- 단순히 단어 등장 횟수를 세는 방식(Bag of Words 등)으로는 '사과'와 '배'가 과일이라는 의미적 유사도를 충분히 반영하기 어려움
- 임베딩 과정을 통해, 서로 의미가 비슷한 단어, 문장이 고차원 벡터 공간 상에서 가까이 위치하도록 학습 시킴
- 임베딩 된 벡터들은 서로 코사인 유사도 등으로 간단히 비교 가능하며, 훨씬 정교한 문서(단어)간 유사도 계산이 가능해짐

## 임베딩 기반 벡터화 순서

1. 데이터 수집
   - 실습에서는 '대한민국 헌법.txt'파일 활용
2. 데이터 전처리
   - 토큰화 : 띄어쓰기(또는 형태소) 기준으로 분리
   - 한글이 아닌 표현 (이모지, 꺽쇠 등)제거
   - 불용어 제거 : 벡터로 만들 필요가 없거나 지나치게 많이 사용되는 단어 제거(조사 등)
3. 임베딩용 딥러닝 모델 설계(Word2Vec, Doc2Vec 등 사용 예정)

## 대표적인 임베딩 기반 벡터화 모델

- Word2Vec
  - 단어 관계를 학습해 각 단어를 의미 벡터로 만드는 방법
- Doc2Vec
  - 문서와 단어 관계를 함께 학습해 문장/문단 전체를 하나의 벡터로 만드는 방법
- 생성형 AI 기반 임베딩
  - 대규모 언어 모델 (LLM)에서 제공하는 임베딩을 활용

# Word2Vec

- 분포 가설에 기반하여 단어의 의미를 벡터로 '학습'
  - 분포 가설 : '서로 비슷한 맥락에서 등장하는 단어들은 의미도 비슷하다'
- 주변 단어를 보고 중심 단어를 맞히거나 학습하면, 단어 간 의미 관계가 유사한 벡터로 학습
- 학습 방식
  - CBOW(continuous BoW) : 주변 단어들을 입력으로 받고(맥락), 그 중심 단어를 예측하는 방식
  - Skip-gram : 중심 단어를 입력으로 받고, 주변 단어들을 예측하는 방식

## Word2Vec 정리

- 분포 가설에 기반하여 단어의 의미를 벡터로 '학습'
  - 분포 가설 : '서로 비슷한 맥락에서 등장하는 단어들은 의미도 비슷하다'
- '단어 벡터 공간'에서 유사 단어끼리는 가깝게, 다른 단어는 멀게 배치해서 유사도 측정
- 학습/추론 속도가 빠르고, 의미적 유사도를 잘 포착하지만
- 문맥별 의미 파악이 불가능
  - ex: 문장 A에서의 탈 것을 의미하는 '차' 와 문장 B에서의 마시는 '차'를 구분하지 못함
- 학습 당시 사용한 말뭉치에 없는 단어는 벡터를 얻을 수 없음

# 도큐먼트 임베딩

- 여러 단어로 이루어진 문장, 문서를 임베딩하는 방법
- 문서별로 고유 문서 태그(문서 ID)를 부여
- Word2Vec과 유사하게 중심 단어와 주변 단어를 예측
  - 문서(문장)를 대표하는 문서 태그를 은닉층에 함께 학습
  - 이 문서 태그 임베딩이 결국 해당 문서를 대표하는 벡터가 됨
- 문서의 단어들을 입력으로 받고(또는 Skip-Gram이면 단어를 예측)
- Doc2Vec 모델을 사용하여 '문서 태그 + 단어 임베딩'을 동시에 업데이트

## Doc2Vec 정리

- 단어 수준(Word2Vec)에서 문단 수준으로 확장하여 단어의 의미를 벡터로 학습하는 기법
- 문서 벡터 공간 에서 유사 문서끼리는 가갑게, 다른 문서는 멀게 배치해서 유사도 측정
- 문서 전체 의미를 직접 반영한 벡터를 획득할 수 있지만
- 문서 수만큼 추가 파라미터가 필요 => 메모리/학습 비용 증가
- 같은 문서 내의 다의어는 구분하지 못함
  - ex: 같은 문서 내에서 탈 것을 의미하는 '차'와 마시는 '차'를 구분하지 못함
- 여전히 학습 당시 사용한 말뭉치에 없는 단어는 벡터를 얻을 수 없음

# 생성형 AI 기반 임베딩

- GPT 계열 같은 대규모 언어 모델(LLM)에서 제공하는 임베딩 기능 활용
  - LLM은 이미 엄청난 텍스트 코퍼스 (말뭉치)로 사전 학습되어 있음
  - 복잡한 자연어 전처리 불필요(토큰화, 불용어 제거등)
- 문장/문서를 입력하면, 모델 내부의 숨은 표현을 활용해 의미가 반영된 벡터를 얻을 수 있음
- API로 텍스트 전달 -> 임베딩 벡터 반환
  - OpenAI, upstage 등
